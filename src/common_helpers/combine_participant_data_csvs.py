from __future__ import annotations

import argparse
import csv
from pathlib import Path
from typing import Sequence


DEFAULT_SOURCE_CSV_PATHS: tuple[str, ...] = (
    "data/elias.csv",
    "data/evan.csv",
    "data/maik.csv",
)

DEFAULT_PARTICIPANT_IDS: tuple[str, ...] = ("P01", "P02", "P03")


def _repo_root() -> Path:
    """Return the repository root inferred from this module location.

    Returns:
        Absolute repository root path.
    """
    return Path(__file__).resolve().parents[2]


def _resolve_path(path_like: str | Path) -> Path:
    """Resolve a path from cwd or repository root.

    Args:
        path_like: Path to resolve.

    Returns:
        Absolute path if the file exists.

    Raises:
        FileNotFoundError: If the path cannot be resolved to an existing file.
    """
    path = Path(path_like)
    if path.exists():
        return path.resolve()

    if not path.is_absolute():
        from_repo_root = (_repo_root() / path).resolve()
        if from_repo_root.exists():
            return from_repo_root

    raise FileNotFoundError(f"Could not find source CSV: {path_like}")


def _read_csv_rows(csv_path: Path) -> tuple[list[str], list[dict[str, str]]]:
    """Read CSV rows and return header + row dictionaries.

    Args:
        csv_path: CSV file path.

    Returns:
        Tuple of header field names and data rows.

    Raises:
        ValueError: If the CSV has no header.
    """
    with csv_path.open("r", newline="", encoding="utf-8") as csv_file:
        reader = csv.DictReader(csv_file)
        fieldnames = list(reader.fieldnames or [])
        if not fieldnames:
            raise ValueError(f"CSV '{csv_path}' has no valid header row.")
        rows = [dict(row) for row in reader]
    return fieldnames, rows


def _validate_source_config(
    source_csv_paths: Sequence[str | Path],
    participant_ids: Sequence[str] | None,
) -> list[tuple[str, Path]]:
    """Validate source CSV and participant ID configuration.

    Args:
        source_csv_paths: CSV paths to combine.
        participant_ids: Optional explicit participant IDs.

    Returns:
        List of `(participant_id, resolved_csv_path)` tuples.

    Raises:
        ValueError: If path/ID lengths do not match or no CSV paths are provided.
    """
    if not source_csv_paths:
        raise ValueError("source_csv_paths must contain at least one CSV path.")

    if participant_ids is None:
        participant_ids = tuple(f"P{index:02d}" for index in range(1, len(source_csv_paths) + 1))

    if len(source_csv_paths) != len(participant_ids):
        raise ValueError(
            "source_csv_paths and participant_ids must have the same length. "
            f"Got {len(source_csv_paths)} and {len(participant_ids)}."
        )

    return [
        (str(pid), _resolve_path(path_like))
        for pid, path_like in zip(participant_ids, source_csv_paths, strict=True)
    ]


def combine_participant_rows(
    source_csv_paths: Sequence[str | Path] = DEFAULT_SOURCE_CSV_PATHS,
    participant_ids: Sequence[str] | None = DEFAULT_PARTICIPANT_IDS,
) -> tuple[list[str], list[dict[str, str]]]:
    """Combine multiple participant CSVs into one row list with participant IDs.

    Args:
        source_csv_paths: Paths to participant CSV files.
        participant_ids: Participant IDs aligned with `source_csv_paths`.
            If None, IDs are autogenerated as `P01`, `P02`, ... in path order.

    Returns:
        Tuple with:
            - output field names (`participant_id` + source columns),
            - merged row dictionaries.

    Raises:
        ValueError: If source schemas differ or inputs are malformed.
        FileNotFoundError: If any source CSV path is missing.
    """
    dataset_config = _validate_source_config(source_csv_paths, participant_ids)

    base_fieldnames: list[str] | None = None
    combined_rows: list[dict[str, str]] = []

    for pid, csv_path in dataset_config:
        current_fieldnames, rows = _read_csv_rows(csv_path)

        if base_fieldnames is None:
            base_fieldnames = current_fieldnames
        elif current_fieldnames != base_fieldnames:
            raise ValueError(
                f"CSV schema mismatch for participant '{pid}'. "
                f"Expected {base_fieldnames}, got {current_fieldnames}."
            )

        for row in rows:
            combined_rows.append({"participant_id": pid, **row})

    output_fieldnames = ["participant_id", *(base_fieldnames or [])]
    return output_fieldnames, combined_rows


def write_dataset_csv(
    output_filename: str | Path = "participants.csv",
    source_csv_paths: Sequence[str | Path] = DEFAULT_SOURCE_CSV_PATHS,
    participant_ids: Sequence[str] | None = DEFAULT_PARTICIPANT_IDS,
) -> Path:
    """Write one merged participant CSV to `<repo>/data`.

    Args:
        output_filename: Output filename (or absolute path).
        source_csv_paths: Source participant CSV paths.
        participant_ids: Participant IDs aligned with source paths.

    Returns:
        Absolute path to the merged output CSV.
    """
    output_fieldnames, combined_rows = combine_participant_rows(
        source_csv_paths=source_csv_paths,
        participant_ids=participant_ids,
    )

    output_path = Path(output_filename)
    if output_path.is_absolute():
        final_output_path = output_path
    else:
        data_dir = _repo_root() / "data"
        data_dir.mkdir(parents=True, exist_ok=True)
        final_output_path = data_dir / output_path

    with final_output_path.open("w", newline="", encoding="utf-8") as csv_file:
        writer = csv.DictWriter(csv_file, fieldnames=output_fieldnames)
        writer.writeheader()
        writer.writerows(combined_rows)

    return final_output_path.resolve()


def _build_arg_parser() -> argparse.ArgumentParser:
    """Build CLI argument parser for dataset combining.

    Returns:
        Configured parser.
    """
    parser = argparse.ArgumentParser(
        description="Merge participant CSV files into one CSV with participant_id."
    )
    parser.add_argument(
        "--output",
        type=str,
        default="participants.csv",
        help="Output filename (relative to <repo>/data) or absolute path.",
    )
    parser.add_argument(
        "--source",
        nargs="+",
        default=list(DEFAULT_SOURCE_CSV_PATHS),
        help="Source CSV paths in participant order.",
    )
    parser.add_argument(
        "--participant-id",
        nargs="+",
        default=list(DEFAULT_PARTICIPANT_IDS),
        help="Participant IDs aligned with --source order.",
    )
    return parser


if __name__ == "__main__":
    args = _build_arg_parser().parse_args()
    written_path = write_dataset_csv(
        output_filename=args.output,
        source_csv_paths=args.source,
        participant_ids=args.participant_id,
    )
    print(f"Wrote CSV to: {written_path}")

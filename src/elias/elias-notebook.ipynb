{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdde1255",
   "metadata": {},
   "source": [
    "# Using functions from `src/evan/glaze.py` in `src/elias`\n",
    "\n",
    "You can reuse the model code from `src/evan` without changing anything in `src/evan`.\n",
    "\n",
    "## 1) Import functions into a notebook/script in `src/elias`\n",
    "```python\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "repo_root = Path.cwd().resolve()\n",
    "if not (repo_root / \"src\").exists() and (repo_root.parent / \"src\").exists():\n",
    "    repo_root = repo_root.parent\n",
    "\n",
    "sys.path.insert(0, str(repo_root / \"src\"))\n",
    "\n",
    "from evan.glaze import (\n",
    "    psi_function,\n",
    "    simulate_trial,\n",
    "    run_simulation_and_plot,\n",
    "    plot_model_comparison,\n",
    ")\n",
    "```\n",
    "\n",
    "## 2) Main functions\n",
    "- `psi_function(L_prev, H)`: hazard-based prior update (Psi).\n",
    "- `simulate_trial(...)`: simulates one trial and returns `reaction_time_ms`, `final_belief`, `decision`, `trajectory`, and `time_points_ms`.\n",
    "- `run_simulation_and_plot(csv_path, block_id=None)`: runs simulations for one/all blocks and generates plots.\n",
    "- `plot_model_comparison(df, params=None)`: compares actual vs predicted decisions/RT/beliefs (for prepared DataFrames).\n",
    "\n",
    "## 3) Minimal examples\n",
    "```python\n",
    "psi = psi_function(L_prev=0.8, H=0.12)\n",
    "\n",
    "trial = simulate_trial(\n",
    "    prev_belief_L=psi,\n",
    "    current_LLR=0.5,\n",
    "    H=0.12,\n",
    "    belief_threshold=1.0,\n",
    "    max_duration_ms=1500,\n",
    "    noise_std=0.7,\n",
    "    decision_time_ms=50.0,\n",
    "    noise_gain=3.5,\n",
    "    stop_on_sat=True,\n",
    ")\n",
    "\n",
    "trial[\"decision\"], trial[\"reaction_time_ms\"]\n",
    "```\n",
    "\n",
    "```python\n",
    "csv_path = repo_root / \"data\" / \"participants.csv\"\n",
    "run_simulation_and_plot(str(csv_path), block_id=None)  # or block_id=3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffa0127",
   "metadata": {},
   "source": [
    "# Wilson & Collins (2019) Guide Applied to Your Project\n",
    "\n",
    "Reference: Wilson RC, Collins AGE (2019), *Ten simple rules for the computational modeling of behavioral data*, eLife 8:e49547, https://doi.org/10.7554/eLife.49547\n",
    "\n",
    "## Critical question first: what changes when moving from one participant to many?\n",
    "\n",
    "The modeling logic is the same, but the **data structure and inference level** must change.\n",
    "\n",
    "1. Do not treat all rows as one long sequence.\n",
    "Your models are sequential (belief depends on previous trials), so each participant must have an independent sequence.\n",
    "\n",
    "2. Add explicit `participant_id`.\n",
    "`data/participants.csv` currently has no `participant_id` column (it has `block_id`, `trial_index`, etc.), so true multi-participant fitting is not yet identifiable from this file alone.\n",
    "\n",
    "3. Reset latent state at boundaries.\n",
    "At minimum reset at participant boundaries. Usually also reset at block boundaries unless task design says otherwise.\n",
    "\n",
    "4. Decide analysis level.\n",
    "- Recommended first: fit each participant separately, then summarize group distributions.\n",
    "- Later: hierarchical model (partial pooling) for more stable individual estimates.\n",
    "\n",
    "5. Evaluation becomes two-level.\n",
    "- Participant-level: fit quality and best model per person.\n",
    "- Group-level: parameter distributions, model prevalence, and uncertainty.\n",
    "\n",
    "This is fully consistent with Wilson & Collins: sequential tasks require careful handling of dependencies, and conclusions should be validated per participant and at group level.\n",
    "\n",
    "---\n",
    "\n",
    "## Your 3-model comparison (clear definitions)\n",
    "\n",
    "1. **Model A: Continuous/DNM + fixed threshold**\n",
    "- Current implementation path: `simulate_trial(..., stop_on_sat=False)`.\n",
    "- Decision when `|L| >= B` (after non-decision delay).\n",
    "\n",
    "2. **Model B: Continuous/DNM + saturation/asymptote rule**\n",
    "- Current implementation path: `simulate_trial(..., stop_on_sat=True)`.\n",
    "- Effective stopping level tied to asymptote magnitude.\n",
    "\n",
    "3. **Model C: DDM/DNM hybrid**\n",
    "- DNM handles across-trial prior belief update.\n",
    "- DDM handles within-trial RT/choice mechanism.\n",
    "\n",
    "Practical hybrid mapping:\n",
    "\n",
    "- `Psi_t = psi_function(L_{t-1}, H_t)`\n",
    "- `v_t = beta_llr * LLR_t + beta_prior * Psi_t`\n",
    "- `z_t = sigmoid(gamma * Psi_t)`\n",
    "- DDM parameters: boundary `a`, non-decision `t0` (plus optional noise scaling)\n",
    "\n",
    "This gives a principled combination: DNM for volatility-sensitive belief carry-over, DDM for RT dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-step pipeline (Wilson & Collins aligned)\n",
    "\n",
    "## Step 1) Design/check experiment-question alignment\n",
    "\n",
    "Wilson & Collins start here for a reason: model quality cannot rescue weak task design.\n",
    "\n",
    "For your case:\n",
    "\n",
    "1. Confirm your scientific question:\n",
    "- Do participants use hazard-sensitive belief updating?\n",
    "- Is RT better explained by continuous attractor dynamics or DDM-like boundary crossing?\n",
    "\n",
    "2. Confirm your data can answer it:\n",
    "- Enough trials per participant/condition?\n",
    "- Enough variation in evidence (`LLR`) and hazard context?\n",
    "- RT quality good enough for DDM comparison?\n",
    "\n",
    "3. Define model-independent signatures first:\n",
    "- Choice vs LLR curves.\n",
    "- RT vs |LLR| curves.\n",
    "- Block/hazard adaptation patterns.\n",
    "\n",
    "If these signatures are absent, model-based inference is usually weak.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2) Design models to reflect competing hypotheses\n",
    "\n",
    "Wilson & Collins emphasize: include serious competitors, not strawmen.\n",
    "\n",
    "For you:\n",
    "\n",
    "1. Keep Model A, B, C all plausible and interpretable.\n",
    "2. Keep parameterizations comparable in complexity where possible.\n",
    "3. Include nuisance terms if needed (e.g., side bias, lapse), because omitting them can distort key parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3) Simulate, simulate, simulate (before fitting real data)\n",
    "\n",
    "Directly from Wilson & Collins: simulate each model across parameter ranges and inspect qualitative behavior.\n",
    "\n",
    "For you:\n",
    "\n",
    "1. Simulate each model on realistic trial structures.\n",
    "- Best: reuse each participantâ€™s observed `LLR`/hazard sequence.\n",
    "\n",
    "2. Vary parameters over broad but realistic ranges.\n",
    "3. Plot model-independent summaries from simulations.\n",
    "4. Check whether models are behaviorally distinguishable in those summaries.\n",
    "\n",
    "If models are not distinguishable in simulation, model comparison on real data will be ambiguous.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4) Fit parameters robustly\n",
    "\n",
    "Wilson & Collins fitting cautions to implement explicitly:\n",
    "\n",
    "1. Ensure finite log-likelihood/objective everywhere in bounds.\n",
    "2. Avoid numerical issues (zeros, infinities, unstable exponentials).\n",
    "3. Use sensible parameter constraints/transforms.\n",
    "4. Use multi-start optimization to avoid local minima.\n",
    "\n",
    "Recommended pragmatic setup:\n",
    "\n",
    "1. Global stage (e.g., differential evolution / random search).\n",
    "2. Local refinement stage (e.g., L-BFGS-B / trust-constr).\n",
    "3. Keep best solution across many starts.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5) Parameter recovery (core step for your next phase)\n",
    "\n",
    "Wilson & Collins: parameter recovery is mandatory before interpreting fit parameters.\n",
    "\n",
    "Recipe:\n",
    "\n",
    "1. Choose true parameters from realistic ranges.\n",
    "- Use ranges from pilot fits or prior literature.\n",
    "\n",
    "2. Generate synthetic data from one model.\n",
    "3. Fit the same model back to synthetic data.\n",
    "4. Compare true vs recovered parameters.\n",
    "\n",
    "Report at least:\n",
    "\n",
    "1. Correlation true vs recovered.\n",
    "2. Calibration slope/intercept (`hat ~ true`).\n",
    "3. Bias and RMSE.\n",
    "4. Pairwise correlations among recovered parameters (to detect trade-offs).\n",
    "\n",
    "Important Wilson & Collins nuance:\n",
    "Recovery can work only in some parameter regimes. Match simulated parameter ranges to ranges that matter for your empirical fits.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6) Model recovery (for 3-model identifiability)\n",
    "\n",
    "Wilson & Collins: model comparison must be validated by model recovery.\n",
    "\n",
    "Procedure:\n",
    "\n",
    "1. Simulate data from each of A/B/C across realistic parameter ranges.\n",
    "2. Fit all three models to each synthetic dataset.\n",
    "3. Select best model by one common criterion.\n",
    "4. Build 3x3 confusion matrix:\n",
    "- Rows: generating model.\n",
    "- Columns: selected best-fitting model.\n",
    "\n",
    "Also compute inversion matrix when needed:\n",
    "- `p(simulated model | fit model)`\n",
    "\n",
    "Goal: strong diagonal in confusion matrix and interpretable inversion probabilities.\n",
    "\n",
    "If diagonal is weak:\n",
    "\n",
    "1. Improve task/model-independent diagnostics.\n",
    "2. Revisit parameter ranges.\n",
    "3. Revisit model definitions (too similar).\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7) Fit real multi-participant data\n",
    "\n",
    "Wilson & Collins recommend model-independent checks first, then fitting.\n",
    "\n",
    "Recommended order for your dataset:\n",
    "\n",
    "1. Add/validate `participant_id`.\n",
    "2. Run participant-wise model-independent analyses.\n",
    "3. Fit A/B/C per participant.\n",
    "4. Compare A/B/C per participant and summarize group-level evidence.\n",
    "5. Verify fitted parameter ranges are inside ranges where recovery was good.\n",
    "\n",
    "If not, rerun recovery in the empirical range before claiming interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8) Validate the winning model (posterior predictive checks)\n",
    "\n",
    "Wilson & Collins: never skip this.\n",
    "\n",
    "For each participant and winning model:\n",
    "\n",
    "1. Simulate behavior using fitted parameters.\n",
    "2. Compare simulated vs real on the same summary plots:\n",
    "- Choice psychometric function.\n",
    "- RT chronometric function.\n",
    "- RT distribution (quantiles/skew).\n",
    "- Block/hazard dynamics.\n",
    "\n",
    "If model wins numerically but fails these checks, treat inference as unreliable.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 9) Analyze latent variables only after validation\n",
    "\n",
    "Then it is appropriate to analyze latent variables (`L_t`, `Psi_t`, drift components, etc.) and link them to behavior/conditions.\n",
    "\n",
    "Do this only for validated winning model(s), to reduce p-hacking risk and over-interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 10) Report transparently (Wilson & Collins reporting guidance)\n",
    "\n",
    "At minimum report:\n",
    "\n",
    "1. Model recovery results (confusion matrix; optionally inversion matrix).\n",
    "2. Number/proportion of participants best fit by each model.\n",
    "3. Parameter distributions (not only means).\n",
    "4. Pairwise parameter correlations (trade-off diagnostics).\n",
    "5. Parameter recovery plots.\n",
    "6. Validation plots from posterior predictive checks.\n",
    "\n",
    "---\n",
    "\n",
    "## Concrete implementation plan in `src/elias` (without touching `src/evan`)\n",
    "\n",
    "1. `model_continuous.py`\n",
    "- Wrap `evan.glaze.simulate_trial` with a mode switch:\n",
    "  - threshold (`stop_on_sat=False`)\n",
    "  - asymptote (`stop_on_sat=True`)\n",
    "\n",
    "2. `model_ddm_hybrid.py`\n",
    "- Implement DDM simulator and DNM-to-DDM mapping (`Psi -> v,z`).\n",
    "\n",
    "3. `fit_models.py`\n",
    "- Shared objective/likelihood and optimization utilities.\n",
    "- Multi-start fitting.\n",
    "\n",
    "4. `generate_synth.py`\n",
    "- Synthetic data generation for A/B/C.\n",
    "\n",
    "5. `recoverability.py`\n",
    "- Parameter recovery metrics/plots.\n",
    "- Model recovery confusion + inversion matrices.\n",
    "\n",
    "6. `analyze_real_data.py`\n",
    "- Participant-wise fitting, model comparison, and validation plots.\n",
    "\n",
    "---\n",
    "\n",
    "## Minimal pseudo-code: participant-wise fitting\n",
    "\n",
    "```python\n",
    "for pid, df_pid in data.groupby(\"participant_id\"):\n",
    "    df_pid = df_pid.sort_values([\"block_id\", \"trial_index\"])\n",
    "\n",
    "    for model in [\"A_cont_threshold\", \"B_cont_asymptote\", \"C_ddm_dnm\"]:\n",
    "        theta_hat = fit_model(df_pid, model)\n",
    "        score = model_score(df_pid, model, theta_hat)\n",
    "        store(pid, model, theta_hat, score)\n",
    "```\n",
    "\n",
    "## Minimal pseudo-code: model recovery matrix\n",
    "\n",
    "```python\n",
    "for gen_model in models:\n",
    "    synth_data = generate_synthetic(gen_model, n_participants=100)\n",
    "\n",
    "    for fit_model in models:\n",
    "        scores = fit_all_participants(synth_data, fit_model)\n",
    "        save_criterion(gen_model, fit_model, scores)\n",
    "\n",
    "confusion = build_confusion_matrix(saved_criteria)\n",
    "inversion = compute_inversion_matrix(confusion)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Common pitfalls this pipeline prevents\n",
    "\n",
    "1. Interpreting parameters without parameter recovery.\n",
    "2. Claiming one model is best without model recovery.\n",
    "3. Using only fit metrics without posterior predictive validation.\n",
    "4. Ignoring participant-level sequence boundaries.\n",
    "5. Comparing models that are not behaviorally distinguishable in simulated data.\n",
    "\n",
    "This is the Wilson & Collins logic, adapted directly to your continuous-vs-DDM comparison.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ORDER OF MODELING STEPS\n",
    "\n",
    "\n",
    "This notebook follows the modeling framework outlined below.\n",
    "1. Conceptualize Model (What do I want to model and why?)\n",
    "2. Build model (figure out equations, write code)\n",
    "3. Fit model to surrogate data (parameter and model recovery)\n",
    "\n",
    "**CHECKPOINT: Only continue if the model and experiment can answer the question in theory, and if parameters and model are recoverable**\n",
    "\n",
    "4. Fit model to participant data(1. parameter and model fit, 2. validate model)\n",
    "\n",
    "**CHECKPOINT: only continue if the model can account for data.**\n",
    "5. latent variable analysis and report results\n",
    "\n",
    "Based on Prof. Musslick’s lecture slides in his class about cognitive modeling in the winter semester 2024.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# MODEL-COMPARISON LOCK (Triangle task; P01–P03; mixed trial availability)\n",
    "\n",
    "## DATA AVAILABILITY NOTE (CURRENT BRANCH)\n",
    "- Merged dataset is built from `data/elias.csv`, `data/evan.csv` (short), and `data/maik.csv`.\n",
    "- Pre-exclusion row counts are fixed to current files:\n",
    "  - `P01`: `160` rows (`4 x 40`)\n",
    "  - `P02`: `147` rows (`block 1 = 27`; blocks 2-4 = 40 each)\n",
    "  - `P03`: `160` rows (`4 x 40`)\n",
    "- Total pre-exclusion rows: `467`.\n",
    "- No participant-trace duplication is present in the current merged data.\n",
    "\n",
    "## Step 1) Conceptualize model and claim boundaries\n",
    "I will conclude only: \"Among these candidate models, under the pre-specified scoring + held-out evaluation + checks, model X provides the best predictive account of choice+RT in this dataset.\"\n",
    "I will NOT conclude: \"X is the true brain model.\"\n",
    "\n",
    "## Hazard input choice and interpretation (locked)\n",
    "- Locked hazard input is `subjective_h_snapshot` (participant-specific estimate).\n",
    "- What this tests: whether model performance improves when conditioning on the participant's inferred volatility belief.\n",
    "- Pro: can capture imperfect hazard learning/adaptation and support a psychological interpretation.\n",
    "- Con: if `subjective_h_snapshot` is derived from the same fitted data without temporal constraints, it can leak flexibility and inflate apparent performance.\n",
    "- Constraint: treat `subjective_h_snapshot` as one-step-ahead / past-only / externally fixed during fitting and evaluation; do not let current-trial outcomes leak into the predictor used at that trial.\n",
    "- Interpretation if it wins: decision dynamics are best explained when the model uses participants' subjective volatility belief.\n",
    "- Required reporting sentence: \"Claims are conditional on the provenance and temporal validity of `subjective_h_snapshot`.\"\n",
    "\n",
    "## Step 2) Build model with fixed data/scoring specification\n",
    "\n",
    "### Data + preprocessing (fixed)\n",
    "- Observables (targets): `choice_t` (binary) and `rt_t` (milliseconds).\n",
    "- Inputs (given to models): `LLR_t`, hazard/block info via `subjective_h_snapshot`, and any DNM-derived `psi_t` if applicable.\n",
    "- Exclusion rules (apply identically to all models):\n",
    "  - drop trials with missing choice or RT\n",
    "  - drop RT < 150 ms or RT > 5000 ms\n",
    "- Units: keep RT in milliseconds everywhere.\n",
    "\n",
    "### Data availability (current file)\n",
    "- Current merged file has `3` participants and `467` rows before exclusions.\n",
    "- After locked exclusions, current retained rows are `456` (`11` removed: `10` RT > 5000 and `1` RT < 150).\n",
    "- Train/test split is defined by `trial_index` cutoff (TRAIN `<= 30`, TEST `>= 31`), not by fixed retained trial counts.\n",
    "- For `P02` block 1 (short block), the split is still valid and yields `17` TRAIN and `10` TEST trials.\n",
    "\n",
    "### Candidate models (fixed)\n",
    "Model A: **DNM + CNM (blockwise threshold)**\n",
    "- DNM provides trial-wise belief/prior quantities (e.g., `psi_t`) from hazard + evidence.\n",
    "- CNM uses a **block-specific threshold** parameter to generate choice+RT distribution.\n",
    "\n",
    "Model B: **DNM + CNM (blockwise asymptote)**\n",
    "- Same DNM inputs.\n",
    "- CNM uses a **block-specific asymptote (non-absorbing stabilization)** parameter to generate choice+RT distribution.\n",
    "\n",
    "Model C: **DNM + DDM (standard bounded diffusion)**\n",
    "- Map DNM outputs to DDM per trial:\n",
    "  - start point: `x0_t = k_z * psi_t`\n",
    "  - drift: `v_t = k_v * LLR_t`\n",
    "  - bounds: +/-a, nondecision time: t0, diffusion scale fixed (see below).\n",
    "\n",
    "(If any mapping differs, that becomes a separate model and must be reported as such.)\n",
    "\n",
    "### Fit vs fixed parameters (locked)\n",
    "Numerical hyperparameters (fixed for all models):\n",
    "- `dt = 1 ms`, `t_max = 5000 ms`, `n_sims_per_trial = 2000`\n",
    "- RNG: seeded and logged (default seed = 0); same seed policy for all models.\n",
    "- RT density for likelihood: histogram density with bin width `20 ms` + epsilon smoothing `1e-12`.\n",
    "\n",
    "Model parameters to fit **per participant** (fit on TRAIN only):\n",
    "- Model A: 4 block params (threshold per block) + `t0` + one evidence/noise gain (single global).\n",
    "- Model B: 4 block params (asymptote per block) + `t0` + one evidence/noise gain (single global).\n",
    "- Model C (DDM): `a`, `t0`, `k_v`, `k_z`; diffusion scale fixed `s=1.0` (identifiability).\n",
    "\n",
    "## Step 3) Surrogate-data recovery checkpoint (SOFT GATE)\n",
    "Recovery protocol:\n",
    "1. Simulate surrogate datasets from each fitted candidate model.\n",
    "2. Refit all candidate models to each surrogate dataset.\n",
    "3. Summarize model-recovery matrix and parameter-recovery behavior.\n",
    "\n",
    "Soft-gate checkpoint rule:\n",
    "- If recovery is weak (models not distinguishable/recoverable above chance), continue to participant fitting and evaluation, but downgrade all winner claims to weak/inconclusive evidence.\n",
    "\n",
    "## Step 4) Fit participant data and compare models\n",
    "### Primary evaluation protocol (locked)\n",
    "Train/test split (forward-chaining; preserves sequential dependence):\n",
    "- For each participant-block, TRAIN = trials 1-30 by `trial_index`, TEST = trials 31-40 by `trial_index` when those indices are present.\n",
    "- Fit parameters on TRAIN only; evaluate scores on TEST only.\n",
    "- Aggregate TEST scores across the 4 blocks per participant.\n",
    "\n",
    "History usage:\n",
    "- PRIMARY scoring uses one-step-ahead prediction: condition on observed history up to t-1 (if a model uses it).\n",
    "- SECONDARY validation uses free-running simulations (model feeds itself its own simulated history).\n",
    "\n",
    "### Primary scoring rule (locked)\n",
    "For each trial t in TEST:\n",
    "- Joint negative log score:\n",
    "  - `L_t = -log p(choice_t) - log p(rt_t | choice_t)`\n",
    "- `p(choice_t)` and `p(rt_t | choice_t)` are estimated from model simulations (same n_sims, dt, seed rules).\n",
    "\n",
    "Total score per participant = sum over TEST trials across all 4 blocks.\n",
    "Report also (but do NOT use for winner selection):\n",
    "- choice-only score = sum `-log p(choice_t)`\n",
    "- RT-only conditional score = sum `-log p(rt_t | choice_t)`\n",
    "\n",
    "Winner selection uses ONLY the joint score.\n",
    "\n",
    "### \"Winner\" vs \"inconclusive\" rules (locked)\n",
    "Per participant:\n",
    "- A model is a clear winner if:\n",
    "  1) it has the best TEST joint score overall, AND\n",
    "  2) it is best in >= 3 of 4 blocks (blockwise consistency), AND\n",
    "  3) block-bootstrap over the 4 blocks gives DeltaScore(best - runner-up) > 0 with 95% CI strictly > 0.\n",
    "Otherwise: \"no clear winner\" for that participant.\n",
    "\n",
    "Group-level (P01-P03):\n",
    "- Only claim a group preference if the same model is a clear winner in >= 2 of 3 participants.\n",
    "Otherwise: report heterogeneity / inconclusive.\n",
    "\n",
    "## Step 5) Latent-variable analysis and reporting\n",
    "Mandatory checks and reporting (run regardless of winner):\n",
    "1) Posterior predictive checks (per participant; per block):\n",
    "   - RT distribution overlay (data vs simulated)\n",
    "   - RT quantiles (10/30/50/70/90%)\n",
    "   - accuracy by block\n",
    "2) Change-point / hazard signatures:\n",
    "   - accuracy and RT near change-point vs later steady-state\n",
    "   - dependence of RT/choice on prior strength `|psi|` (if DNM present)\n",
    "3) Latent-variable analysis:\n",
    "   - report interpretable latent trajectories/quantities used by the winning or tied models\n",
    "4) Recovery-aware conclusion:\n",
    "   - if surrogate recovery is weak, downgrade any winner statement to weak/inconclusive evidence.\n",
    "\n",
    "Interpretation mapping:\n",
    "- If Model A wins: bounded/thresholded continuous accumulation with blockwise caution provides the best predictive account.\n",
    "- If Model B wins: non-absorbing stabilization/asymptote mechanism better captures behavior than strict bound crossing.\n",
    "- If Model C wins: standard DDM driven by trial-wise prior (start) + evidence (drift) is sufficient; extra CNM nonlinearity is not supported here.\n",
    "- If inconclusive: dataset does not disambiguate these mechanisms under the locked protocol; report equivalence and required future data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Implementation, partly still: Implementation Plan (Detailed; Must Follow MODEL-COMPARISON LOCK)\n",
    "\n",
    "This section is the executable plan and must follow the `MODEL-COMPARISON LOCK` order exactly.\n",
    "\n",
    "## DATA AVAILABILITY NOTE (CURRENT BRANCH)\n",
    "- Current source CSVs are `data/elias.csv`, `data/evan.csv` (short), and `data/maik.csv`.\n",
    "- Current merged dataset contains `467` rows before exclusions and `456` rows after exclusions.\n",
    "- `P02` has a short block 1 (`27` observed trials by index), while all other participant-blocks contain `40` rows before exclusions.\n",
    "\n",
    "## 0) [x] Scope and constraints\n",
    "- [x] Scope is non-hierarchical, participant-wise fitting (`P01`, `P02`, `P03`).\n",
    "- [x] Compare exactly 3 candidate models:\n",
    "  1. Model A: DNM + CNM (blockwise threshold)\n",
    "  2. Model B: DNM + CNM (blockwise asymptote)\n",
    "  3. Model C: DNM + DDM (start from `psi`, drift from `LLR`)\n",
    "- [x] Primary target is joint prediction of `choice` and `rt`.\n",
    "- [x] Winner selection uses TEST joint score only (as locked).\n",
    "\n",
    "## 1) Step 2 build phase: notebook setup and reproducibility [x]\n",
    "- [x] Add one setup code cell that:\n",
    "  1. Loads `src/elias/elias_ddm.py`.\n",
    "  2. Imports `numpy`, `pandas`, `matplotlib`, `scipy`.\n",
    "  3. Sets global constants from lock:\n",
    "     - `DT = 1` (ms)\n",
    "     - `T_MAX = 5000` (ms)\n",
    "     - `N_SIMS_PER_TRIAL = 2000`\n",
    "     - `RT_BIN_WIDTH = 20` (ms)\n",
    "     - `EPS = 1e-12`\n",
    "     - `SEED = 0`\n",
    "  4. Initializes deterministic RNG policy (fixed seeds logged per participant/model).\n",
    "\n",
    "## 2) Step 2 build phase: data prep, scoring interface, and parameterization\n",
    "- [x] Data load, preprocessing overview, exclusions, and split.\n",
    "- [ ] Unified simulation-to-likelihood interface:\n",
    "  1. `p(choice_t)`\n",
    "  2. `p(rt_t | choice_t)` via histogram density (`20 ms` bins + `EPS` smoothing)\n",
    "  3. joint NLL `L_t = -log p(choice_t) - log p(rt_t | choice_t)`\n",
    "  4. aggregate outputs: joint (primary), choice-only, RT-only\n",
    "- [ ] Model parameterization and bounds (participant-wise):\n",
    "  1. Model A: `theta_A = [thr_b1, thr_b2, thr_b3, thr_b4, t0, g]`\n",
    "  2. Model B: `theta_B = [asy_b1, asy_b2, asy_b3, asy_b4, t0, g]`\n",
    "  3. Model C: `theta_C = [a, t0, k_v, k_z]`, `s = 1.0`\n",
    "\n",
    "## 3) Step 3 surrogate-data recovery checkpoint (SOFT GATE)\n",
    "- [ ] Simulate surrogate datasets from each fitted model.\n",
    "- [ ] Refit all candidate models to each surrogate dataset.\n",
    "- [ ] Build model-recovery matrix and parameter-recovery summary.\n",
    "- [ ] Apply soft-gate rule:\n",
    "  - if recovery is weak, continue to participant TEST evaluation,\n",
    "  - but force interpretation to weak/inconclusive evidence.\n",
    "\n",
    "## 4) Step 4 participant fitting and held-out model comparison\n",
    "- [ ] Use one optimizer protocol for all models:\n",
    "  1. same number of multi-starts\n",
    "  2. same convergence criteria and max iterations\n",
    "  3. same seed policy for simulation-based likelihood evaluation\n",
    "- [ ] Fit independently for each participant on TRAIN only.\n",
    "- [ ] Save full fit artifacts:\n",
    "  - best params\n",
    "  - best TRAIN joint score\n",
    "  - optimizer status and iterations\n",
    "  - per-start results\n",
    "- [ ] Evaluate fitted params on TEST only.\n",
    "- [ ] Compute per participant:\n",
    "  1. TEST joint score by block\n",
    "  2. TEST joint score total\n",
    "  3. TEST choice-only and RT-only totals (secondary)\n",
    "- [ ] Apply locked participant-level winner rule.\n",
    "- [ ] Apply locked group rule.\n",
    "\n",
    "## 5) Step 5 latent-variable analysis and reporting\n",
    "- [ ] Posterior predictive checks per participant and block:\n",
    "  1. RT distribution overlay\n",
    "  2. RT quantile comparison (10/30/50/70/90)\n",
    "  3. accuracy by block\n",
    "- [ ] Change-point/hazard signatures:\n",
    "  1. RT and accuracy near change-point vs late block\n",
    "  2. dependence on prior strength `|psi|`\n",
    "- [ ] Latent-variable analysis for interpretability.\n",
    "- [ ] Discussion text for hazard-input interpretation (`subjective_h_snapshot`):\n",
    "  - what it tests\n",
    "  - benefits\n",
    "  - leakage risk and temporal-validity caveat\n",
    "  - interpretation if it wins\n",
    "  - required sentence: \"Claims are conditional on the provenance and temporal validity of `subjective_h_snapshot`.\"\n",
    "\n",
    "## 6) Deliverables and export\n",
    "- [ ] Export participant-level result table:\n",
    "  - fitted params per model\n",
    "  - TRAIN and TEST scores\n",
    "  - winner/inconclusive status\n",
    "- [ ] Export block-level table for bootstrap and consistency checks.\n",
    "- [ ] Save all diagnostic plots used in the report.\n",
    "- [ ] Write concise conclusion text following lock interpretation mapping.\n",
    "\n",
    "## 7) Definition of done\n",
    "- [ ] Lock constraints are satisfied in the declared order (Steps 1->5).\n",
    "- [ ] Surrogate soft-gate decision is documented and propagated to conclusions.\n",
    "- [ ] Winner decision is reproducible from saved tables and seed logs.\n",
    "- [ ] Mandatory checks are present for every participant.\n",
    "- [ ] Hazard-input caveats are explicitly reported in the discussion section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import importlib\n",
    "import sys\n",
    "from typing import Final\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# I resolve the repository root so the notebook works from different launch folders.\n",
    "REPO_ROOT = Path.cwd().resolve().parent.parent\n",
    "if not (REPO_ROOT / \"src\").exists() and (REPO_ROOT.parent / \"src\").exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "# I add both `src/` and `src/elias/` to the import path for shared + model modules.\n",
    "SRC_ROOT = REPO_ROOT / \"src\"\n",
    "ELIAS_SRC = SRC_ROOT / \"elias\"\n",
    "for import_path in (SRC_ROOT, ELIAS_SRC):\n",
    "    if str(import_path) not in sys.path:\n",
    "        sys.path.insert(0, str(import_path))\n",
    "\n",
    "# I reload local modules so notebook edits are picked up without stale imports.\n",
    "import common_helpers.preprocessing as _preprocessing\n",
    "import elias_ddm as _elias_ddm\n",
    "importlib.reload(_preprocessing)\n",
    "importlib.reload(_elias_ddm)\n",
    "\n",
    "# I import shared preprocessing helpers from common_helpers.\n",
    "from common_helpers.preprocessing import load_participant_data, preprocess_loaded_participant_data\n",
    "\n",
    "# I import the model runners from elias_ddm.\n",
    "from elias_ddm import (\n",
    "    run_all_models_for_participant,\n",
    "    run_model_a_threshold,\n",
    "    run_model_b_asymptote,\n",
    "    run_model_c_ddm,\n",
    ")\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4) Locked hard-coded constants (must match MODEL-COMPARISON LOCK).\n",
    "# Keep these fixed across models/participants for fair comparison.\n",
    "# `elias_ddm` APIs now use milliseconds directly.\n",
    "\n",
    "\n",
    "# Simulation integration step (DT = delta time) in milliseconds.\n",
    "DT: Final[float] = 1\n",
    "\n",
    "# Maximum allowed RT window per trial in milliseconds.\n",
    "T_MAX: Final[float] = 5000\n",
    "\n",
    "# Monte Carlo samples per trial for probability estimates.\n",
    "N_SIMS_PER_TRIAL: Final[int] = 2000\n",
    "\n",
    "# RT histogram bin width in ms for density scoring.\n",
    "RT_BIN_WIDTH: Final[float] = 20\n",
    "\n",
    "# Small smoothing constant (EPS=epsilon) to avoid log(0) in likelihood terms.\n",
    "EPS: Final[float] = 1e-12\n",
    "\n",
    "# Global base RNG seed for reproducible simulation/scoring.\n",
    "SEED: Final[int] = 0\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "print(\"Setup complete.\")\n",
    "print(f\"REPO_ROOT = {REPO_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2) Data load, preprocessing overview, exclusions, and split [x]\n",
    "\n",
    "- [x] Add one data-prep code cell that:\n",
    "  - [x] Loads all participants with `load_participant_data(...)`.\n",
    "  - [x] Applies locked exclusions:\n",
    "     - drop missing `choice` or RT\n",
    "     - drop RT < 150 ms\n",
    "     - drop RT > 5000 ms\n",
    "  - [x] Verifies expected structure per participant in the current dataset:\n",
    "     - 3 participants and 4 blocks each\n",
    "     - `P01`: 40/40/40/40 rows before exclusions\n",
    "     - `P02`: 27/40/40/40 rows before exclusions (short block 1)\n",
    "     - `P03`: 40/40/40/40 rows before exclusions\n",
    "  - [x] Creates split labels per block using trial index:\n",
    "     - TRAIN: `trial_index <= 30`\n",
    "     - TEST: `trial_index >= 31`\n",
    "  - [x] Saves preprocessing overview table (`participant`, `block`, `n_train`, `n_test`, `n_dropped`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading, exclusions, preprocessing overview table, and split labels\n",
    "\n",
    "# I use the existing merged participant CSV that is already stored in `data/`.\n",
    "participants_csv_path = REPO_ROOT / \"data\" / \"participants.csv\"\n",
    "\n",
    "# I load all participant rows and derive model-ready state columns.\n",
    "df_loaded = load_participant_data(\n",
    "    csv_path=participants_csv_path,\n",
    "    participant_ids=None,\n",
    "    hazard_col=\"subjective_h_snapshot\",\n",
    "    reset_on=(\"participant\", \"block\"),\n",
    ")\n",
    "\n",
    "# I apply one shared preprocessing helper to keep this notebook cell simple.\n",
    "prep_outputs = preprocess_loaded_participant_data(\n",
    "    df_loaded,\n",
    "    min_rt_ms=150,\n",
    "    max_rt_ms=5000,\n",
    "    train_trial_max_index=30,\n",
    "    expected_blocks_per_participant=4,\n",
    "    nominal_trials_per_block_before=40,\n",
    ")\n",
    "\n",
    "# I unpack outputs used in later notebook sections.\n",
    "df_all = prep_outputs[\"df_all\"]\n",
    "removed_rows_df = prep_outputs[\"removed_rows_df\"]\n",
    "preprocessing_overview_table = prep_outputs[\"preprocessing_overview_table\"]\n",
    "participant_structure_table = prep_outputs[\"participant_structure_table\"]\n",
    "\n",
    "# I report whether this safety step changed anything.\n",
    "print(f\"Participants CSV path:    {participants_csv_path}\")\n",
    "print(f\"Rows before safety check: {prep_outputs['before_n']}\")\n",
    "print(f\"Rows after safety check:  {prep_outputs['after_n']}\")\n",
    "print(f\"Rows removed:             {prep_outputs['removed_n']}\")\n",
    "print(f\"Safety check changed data: {prep_outputs['safety_check_changed_data']}\")\n",
    "\n",
    "# I show participants still present after exclusions.\n",
    "print(\"\\nParticipants:\\n\", df_all[\"participant_id\"].unique())\n",
    "\n",
    "# I show participant-level structure checks requested in section 2.\n",
    "print(\"\\nParticipant structure checks:\")\n",
    "display(participant_structure_table)\n",
    "\n",
    "\n",
    "\n",
    "# I show the preprocessing overview table requested in section 2.\n",
    "print(\"\\nPreprocessing overview table:\")\n",
    "display(preprocessing_overview_table)\n",
    "\n",
    "# I show all rows removed by validity/RT filters for auditability.\n",
    "print(\"\\nRemoved rows (all rows excluded by validity/RT checks):\")\n",
    "display(removed_rows_df)\n",
    "\n",
    "# I show the prepared modeling DataFrame used by downstream cells.\n",
    "print(\"\\nPrepared modeling DataFrame (head):\")\n",
    "display(df_all.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 2) RESULTS:\n",
    "\n",
    "- Using the current merged file `data/participants.csv`, the dataset contains `3` participants with `467` rows before exclusions:\n",
    "  - `P01`: `160` rows\n",
    "  - `P02`: `147` rows (short block 1)\n",
    "  - `P03`: `160` rows\n",
    "- Block structure before exclusions:\n",
    "  - `P01`: `40/40/40/40`\n",
    "  - `P02`: `27/40/40/40`\n",
    "  - `P03`: `40/40/40/40`\n",
    "- After locked exclusions (`drop missing required values`, `RT < 150 ms`, `RT > 5000 ms`), `456` rows remain and `11` rows are removed.\n",
    "- Removal reasons in current data:\n",
    "  - `10` rows with `RT > 5000 ms`\n",
    "  - `1` row with `RT < 150 ms`\n",
    "  - `0` rows dropped due to missing/non-finite required values\n",
    "- Participant-level counts after exclusions:\n",
    "  - `P01`: `149/160` kept (`11` dropped)\n",
    "  - `P02`: `147/147` kept (`0` dropped)\n",
    "  - `P03`: `160/160` kept (`0` dropped)\n",
    "- Split consequences with trial-index rule:\n",
    "  - `P02` block 1 yields `17` TRAIN and `10` TEST trials (valid, but fewer TRAIN trials than full blocks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 2) Build step: unified simulation-to-likelihood interface\n",
    "\n",
    "- [ ] Add helper code cells implementing one common scoring interface for all models.\n",
    "- [ ] For each trial, estimate from simulations:\n",
    "  1. `p(choice_t)`\n",
    "  2. `p(rt_t | choice_t)` via histogram density (`20 ms` bins + `EPS` smoothing)\n",
    "- [ ] Trial joint negative log score:\n",
    "  - `L_t = -log p(choice_t) - log p(rt_t | choice_t)`\n",
    "- [ ] Return all three aggregates:\n",
    "  1. joint score (primary)\n",
    "  2. choice-only score\n",
    "  3. RT-only conditional score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### 3) RESULTS (Surrogate recovery and checkpoint):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 2) Build step: model parameterization and bounds (participant-wise)\n",
    "\n",
    "- [ ] Define explicit parameter vectors and transforms.\n",
    "\n",
    "### Model A parameters (fit on TRAIN)\n",
    "- [ ] `theta_A = [thr_b1, thr_b2, thr_b3, thr_b4, t0, g]`\n",
    "- [ ] Constrain thresholds positive and `t0` in valid range.\n",
    "- [ ] `g` is a global evidence/noise gain for participant.\n",
    "\n",
    "### Model B parameters (fit on TRAIN)\n",
    "- [ ] `theta_B = [asy_b1, asy_b2, asy_b3, asy_b4, t0, g]`\n",
    "- [ ] Constrain asymptotes positive and `t0` in valid range.\n",
    "\n",
    "### Model C parameters (fit on TRAIN)\n",
    "- [ ] `theta_C = [a, t0, k_v, k_z]`\n",
    "- [ ] Fix diffusion scale `s = 1.0`.\n",
    "- [ ] Use mapping from lock:\n",
    "  - `x0_t = k_z * psi_t`\n",
    "  - `v_t = k_v * LLR_t`\n",
    "\n",
    "## 3) Surrogate-data recovery checkpoint (SOFT GATE)\n",
    "\n",
    "- [ ] Simulate surrogate datasets from each fitted model.\n",
    "- [ ] Refit all candidate models to each surrogate dataset.\n",
    "- [ ] Summarize recovery matrix and distinguishability.\n",
    "- [ ] Apply soft-gate interpretation rule:\n",
    "  - if recovery is weak, continue analysis but downgrade winner claims to weak/inconclusive evidence.\n",
    "\n",
    "## 4) Fit participant data and compare models on TEST\n",
    "\n",
    "### 4a) Fitting procedure (TRAIN only)\n",
    "- [ ] Use one optimizer protocol for all models:\n",
    "  1. Multi-start optimization (same number of starts per model).\n",
    "  2. Same convergence criteria and max iterations.\n",
    "  3. Same seed policy for simulation-based likelihood evaluation.\n",
    "- [ ] Fit independently for each participant.\n",
    "- [ ] Save full fit artifacts:\n",
    "  - best params\n",
    "  - best TRAIN joint score\n",
    "  - optimizer status and iterations\n",
    "  - per-start results\n",
    "\n",
    "### 4b) TEST evaluation and winner decision\n",
    "- [ ] Evaluate fitted params on TEST only.\n",
    "- [ ] Compute per participant:\n",
    "  1. TEST joint score by block\n",
    "  2. TEST joint score total (sum over 4 blocks)\n",
    "  3. TEST choice-only and RT-only totals (secondary)\n",
    "- [ ] Apply locked participant-level winner rule:\n",
    "  1. best TEST joint total\n",
    "  2. best in >= 3/4 blocks\n",
    "  3. block-bootstrap CI for DeltaScore(best - runner-up) strictly > 0\n",
    "- [ ] Apply locked group rule:\n",
    "  - same model is clear winner in >= 2/3 participants\n",
    "  - otherwise report heterogeneity/inconclusive\n",
    "\n",
    "## 5) Latent-variable analysis, validity checks, and reporting\n",
    "\n",
    "- [ ] Posterior predictive checks per participant and block:\n",
    "  1. RT distribution overlay (data vs simulated)\n",
    "  2. RT quantile comparison (10/30/50/70/90)\n",
    "  3. accuracy by block\n",
    "- [ ] Change-point/hazard signatures:\n",
    "  1. RT and accuracy near change-point vs late block\n",
    "  2. dependence on prior strength `|psi|`\n",
    "- [ ] Latent-variable analysis and interpretation.\n",
    "- [ ] Discussion caveat for hazard-input choice (`subjective_h_snapshot`):\n",
    "  - include leakage/circularity risk and temporal-validity constraint\n",
    "  - include required sentence template from lock\n",
    "\n",
    "## 6) Deliverables and export\n",
    "\n",
    "- [ ] Export participant-level result table:\n",
    "  - fitted params per model\n",
    "  - TRAIN and TEST scores\n",
    "  - winner/inconclusive status\n",
    "- [ ] Export block-level table for bootstrap and consistency checks.\n",
    "- [ ] Save all diagnostic plots used in report.\n",
    "- [ ] Write concise conclusion text strictly following lock interpretation mapping.\n",
    "\n",
    "## 7) Definition of done\n",
    "\n",
    "- [ ] All lock constraints are satisfied in order (Step 1 -> Step 5).\n",
    "- [ ] Surrogate soft-gate outcome is documented and propagated into conclusions.\n",
    "- [ ] Winner decision is reproducible from saved tables and seed logs.\n",
    "- [ ] Mandatory validity checks are present for every participant.\n",
    "- [ ] If model recovery is weak, final claim is explicitly downgraded to weak/inconclusive evidence.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

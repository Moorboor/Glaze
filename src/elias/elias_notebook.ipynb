{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ORDER OF MODELING STEPS\n",
    "\n",
    "\n",
    "This notebook follows the modeling framework outlined below.\n",
    "1. Conceptualize Model (What do I want to model and why?)\n",
    "2. Build model (figure out equations, write code)\n",
    "3. Fit model to surrogate data (parameter and model recovery)\n",
    "\n",
    "**CHECKPOINT: Only continue if the model and experiment can answer the question in theory, and if parameters and model are recoverable**\n",
    "\n",
    "4. Fit model to participant data(1. parameter and model fit, 2. validate model)\n",
    "\n",
    "**CHECKPOINT: only continue if the model can account for data.**\n",
    "5. latent variable analysis and report results\n",
    "\n",
    "Based on Prof. Musslick’s lecture slides in his class about cognitive modeling in the winter semester 2024.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Implementation, partly still: Implementation Plan (Detailed; Must Follow MODEL-COMPARISON LOCK)\n",
    "\n",
    "This section is the executable plan for the analysis workflow. The `MODEL-COMPARISON LOCK` section is the source of truth for assumptions, scoring, and inference rules.\n",
    "\n",
    "## 0) Scope and constraints [x]\n",
    "\n",
    "- [x] Scope is non-hierarchical, participant-wise fitting (`P01`, `P02`, `P03`).\n",
    "- [x] Compare exactly 3 candidate models:\n",
    "  1. Model A: DNM + CNM (blockwise threshold)\n",
    "  2. Model B: DNM + CNM (blockwise asymptote)\n",
    "  3. Model C: DNM + DDM (start from `psi`, drift from `LLR`)\n",
    "- [x] Primary target is joint prediction of `choice` and `rt`.\n",
    "- [x] Winner selection uses TEST joint score only (as locked).\n",
    "\n",
    "## 1) Notebook setup and reproducibility [x]\n",
    "\n",
    "- [x] Add one setup code cell that:\n",
    "  1. Loads `src/elias/elias_ddm.py`.\n",
    "  2. Imports `numpy`, `pandas`, `matplotlib`, `scipy`.\n",
    "  3. Sets global constants from lock:\n",
    "     - `DT = 1` (ms)\n",
    "     - `T_MAX = 5000` (ms)\n",
    "     - `N_SIMS_PER_TRIAL = 2000`\n",
    "     - `RT_BIN_WIDTH = 20` (ms)\n",
    "     - `EPS = 1e-12`\n",
    "     - `SEED = 0`\n",
    "  4. Initializes deterministic RNG policy (fixed seeds logged per participant/model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import importlib\n",
    "import sys\n",
    "from typing import Final\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# I resolve the repository root so the notebook works from different launch folders.\n",
    "REPO_ROOT = Path.cwd().resolve().parent.parent\n",
    "if not (REPO_ROOT / \"src\").exists() and (REPO_ROOT.parent / \"src\").exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "# I add both `src/` and `src/elias/` to the import path for shared + model modules.\n",
    "SRC_ROOT = REPO_ROOT / \"src\"\n",
    "ELIAS_SRC = SRC_ROOT / \"elias\"\n",
    "for import_path in (SRC_ROOT, ELIAS_SRC):\n",
    "    if str(import_path) not in sys.path:\n",
    "        sys.path.insert(0, str(import_path))\n",
    "\n",
    "# I reload local modules so notebook edits are picked up without stale imports.\n",
    "import common_helpers.preprocessing as _preprocessing\n",
    "import elias_ddm as _elias_ddm\n",
    "importlib.reload(_preprocessing)\n",
    "importlib.reload(_elias_ddm)\n",
    "\n",
    "# I import shared preprocessing helpers from common_helpers.\n",
    "from common_helpers.preprocessing import load_participant_data, preprocess_loaded_participant_data\n",
    "\n",
    "# I import the model runners from elias_ddm.\n",
    "from elias_ddm import (\n",
    "    run_all_models_for_participant,\n",
    "    run_model_a_threshold,\n",
    "    run_model_b_asymptote,\n",
    "    run_model_c_ddm,\n",
    ")\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4) Locked hard-coded constants (must match MODEL-COMPARISON LOCK).\n",
    "# Keep these fixed across models/participants for fair comparison.\n",
    "# `elias_ddm` APIs now use milliseconds directly.\n",
    "\n",
    "\n",
    "# Simulation integration step (DT = delta time) in milliseconds.\n",
    "DT: Final[float] = 1\n",
    "\n",
    "# Maximum allowed RT window per trial in milliseconds.\n",
    "T_MAX: Final[float] = 5000\n",
    "\n",
    "# Monte Carlo samples per trial for probability estimates.\n",
    "N_SIMS_PER_TRIAL: Final[int] = 2000\n",
    "\n",
    "# RT histogram bin width in ms for density scoring.\n",
    "RT_BIN_WIDTH: Final[float] = 20\n",
    "\n",
    "# Small smoothing constant (EPS=epsilon) to avoid log(0) in likelihood terms.\n",
    "EPS: Final[float] = 1e-12\n",
    "\n",
    "# Global base RNG seed for reproducible simulation/scoring.\n",
    "SEED: Final[int] = 0\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "print(\"Setup complete.\")\n",
    "print(f\"REPO_ROOT = {REPO_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2) Data load, preprocessing overview, exclusions, and split [x]\n",
    "\n",
    "- [x] Add one data-prep code cell that:\n",
    "  - [x] Loads all participants with `load_participant_data(...)`.\n",
    "  - [x] Applies locked exclusions:\n",
    "     - drop missing `choice` or RT\n",
    "     - drop RT < 150 ms\n",
    "     - drop RT > 5000 ms\n",
    "  - [x] Verifies expected structure per participant:\n",
    "     - 4 blocks\n",
    "     - nominally 40 trials per block before exclusions\n",
    "  - [x] Creates split labels per block:\n",
    "     - TRAIN: trials 1-30\n",
    "     - TEST: trials 31-40\n",
    "  - [x] Saves preprocessing overview table (`participant`, `block`, `n_train`, `n_test`, `n_dropped`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading, exclusions, preprocessing overview table, and split labels\n",
    "\n",
    "# I use the existing merged participant CSV that is already stored in `data/`.\n",
    "participants_csv_path = REPO_ROOT / \"data\" / \"participants.csv\"\n",
    "\n",
    "# I load all participant rows and derive model-ready state columns.\n",
    "df_loaded = load_participant_data(\n",
    "    csv_path=participants_csv_path,\n",
    "    participant_ids=None,\n",
    "    hazard_col=\"subjective_h_snapshot\",\n",
    "    reset_on=(\"participant\", \"block\"),\n",
    ")\n",
    "\n",
    "# I apply one shared preprocessing helper to keep this notebook cell simple.\n",
    "prep_outputs = preprocess_loaded_participant_data(\n",
    "    df_loaded,\n",
    "    min_rt_ms=150,\n",
    "    max_rt_ms=5000,\n",
    "    train_trial_max_index=30,\n",
    "    expected_blocks_per_participant=4,\n",
    "    nominal_trials_per_block_before=40,\n",
    ")\n",
    "\n",
    "# I unpack outputs used in later notebook sections.\n",
    "df_all = prep_outputs[\"df_all\"]\n",
    "removed_rows_df = prep_outputs[\"removed_rows_df\"]\n",
    "preprocessing_overview_table = prep_outputs[\"preprocessing_overview_table\"]\n",
    "participant_structure_table = prep_outputs[\"participant_structure_table\"]\n",
    "\n",
    "# I report whether this safety step changed anything.\n",
    "print(f\"Participants CSV path:    {participants_csv_path}\")\n",
    "print(f\"Rows before safety check: {prep_outputs['before_n']}\")\n",
    "print(f\"Rows after safety check:  {prep_outputs['after_n']}\")\n",
    "print(f\"Rows removed:             {prep_outputs['removed_n']}\")\n",
    "print(f\"Safety check changed data: {prep_outputs['safety_check_changed_data']}\")\n",
    "\n",
    "# I show participants still present after exclusions.\n",
    "print(\"\\nParticipants:\\n\", df_all[\"participant_id\"].unique())\n",
    "\n",
    "# I show participant-level structure checks requested in section 2.\n",
    "print(\"\\nParticipant structure checks:\")\n",
    "display(participant_structure_table)\n",
    "# I build one merged participant CSV from the three stored source CSV files.\n",
    "combined_csv_path = write_dataset_csv(\n",
    "    output_filename=\"participants.csv\",\n",
    "    source_csv_paths=(\n",
    "        REPO_ROOT / \"data\" / \"elias-standard.csv\",\n",
    "        REPO_ROOT / \"data\" / \"evan-standard.csv\",\n",
    "        REPO_ROOT / \"data\" / \"maik-standard.csv\",\n",
    "    ),\n",
    "    participant_ids=(\"P01\", \"P02\", \"P03\"),\n",
    ")\n",
    "# I show the preprocessing overview table requested in section 2.\n",
    "print(\"\\nPreprocessing overview table:\")\n",
    "display(preprocessing_overview_table)\n",
    "\n",
    "# I show all rows removed by validity/RT filters for auditability.\n",
    "print(\"\\nRemoved rows (all rows excluded by validity/RT checks):\")\n",
    "display(removed_rows_df)\n",
    "\n",
    "# I show the prepared modeling DataFrame used by downstream cells.\n",
    "print(\"\\nPrepared modeling DataFrame (head):\")\n",
    "display(df_all.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 2) RESULTS:\n",
    "\n",
    "- P01 has only 27 instead of 40 trials some might have gone missing, likely due\n",
    "to an error partly caused by the strange csv-string saving method\n",
    "- 11 Trials of participant 3 where dropped because he (I think that was me,\n",
    "very unscientific, that I know this), responded either too quickly or to slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3) Unified simulation-to-likelihood interface\n",
    "\n",
    "- [ ] Add helper code cells implementing one common scoring interface for all models.\n",
    "- [ ] For each trial, estimate from simulations:\n",
    "  1. `p(choice_t)`\n",
    "  2. `p(rt_t | choice_t)` via histogram density (`20 ms` bins + `EPS` smoothing)\n",
    "- [ ] Trial joint negative log score:\n",
    "  - `L_t = -log p(choice_t) - log p(rt_t | choice_t)`\n",
    "- [ ] Return all three aggregates:\n",
    "  1. joint score (primary)\n",
    "  2. choice-only score\n",
    "  3. RT-only conditional score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 3) RESULTS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4) Model parameterization and bounds (participant-wise)\n",
    "\n",
    "- [ ] Define explicit parameter vectors and transforms.\n",
    "\n",
    "### Model A parameters (fit on TRAIN)\n",
    "- [ ] `theta_A = [thr_b1, thr_b2, thr_b3, thr_b4, t0, g]`\n",
    "- [ ] Constrain thresholds positive and `t0` in valid range.\n",
    "- [ ] `g` is a global evidence/noise gain for participant.\n",
    "\n",
    "### Model B parameters (fit on TRAIN)\n",
    "- [ ] `theta_B = [asy_b1, asy_b2, asy_b3, asy_b4, t0, g]`\n",
    "- [ ] Constrain asymptotes positive and `t0` in valid range.\n",
    "\n",
    "### Model C parameters (fit on TRAIN)\n",
    "- [ ] `theta_C = [a, t0, k_v, k_z]`\n",
    "- [ ] Fix diffusion scale `s = 1.0`.\n",
    "- [ ] Use mapping from lock:\n",
    "  - `x0_t = k_z * psi_t`\n",
    "  - `v_t = k_v * LLR_t`\n",
    "\n",
    "## 5) Fitting procedure (TRAIN only)\n",
    "\n",
    "- [ ] Use one optimizer protocol for all models:\n",
    "  1. Multi-start optimization (same number of starts per model).\n",
    "  2. Same convergence criteria and max iterations.\n",
    "  3. Same seed policy for simulation-based likelihood evaluation.\n",
    "- [ ] Fit independently for each participant.\n",
    "- [ ] Save full fit artifacts:\n",
    "  - best params\n",
    "  - best TRAIN joint score\n",
    "  - optimizer status and iterations\n",
    "  - per-start results\n",
    "\n",
    "## 6) TEST evaluation and winner decision\n",
    "\n",
    "- [ ] Evaluate fitted params on TEST only.\n",
    "- [ ] Compute per participant:\n",
    "  1. TEST joint score by block\n",
    "  2. TEST joint score total (sum over 4 blocks)\n",
    "  3. TEST choice-only and RT-only totals (secondary)\n",
    "- [ ] Apply locked participant-level winner rule:\n",
    "  1. best TEST joint total\n",
    "  2. best in >= 3/4 blocks\n",
    "  3. block-bootstrap CI for DeltaScore(best - runner-up) strictly > 0\n",
    "- [ ] Apply locked group rule:\n",
    "  - same model is clear winner in >= 2/3 participants\n",
    "  - otherwise report heterogeneity/inconclusive\n",
    "\n",
    "## 7) Mandatory validity checks (after winner logic)\n",
    "\n",
    "- [ ] Posterior predictive checks per participant and block:\n",
    "  1. RT distribution overlay (data vs simulated)\n",
    "  2. RT quantile comparison (10/30/50/70/90)\n",
    "  3. accuracy by block\n",
    "- [ ] Change-point/hazard signatures:\n",
    "  1. RT and accuracy near change-point vs late block\n",
    "  2. dependence on prior strength `|psi|`\n",
    "- [ ] Model recovery:\n",
    "  1. simulate surrogate data from each fitted model\n",
    "  2. refit all models\n",
    "  3. summarize recovery matrix and distinguishability\n",
    "\n",
    "## 8) Deliverables and export\n",
    "\n",
    "- [ ] Export participant-level result table:\n",
    "  - fitted params per model\n",
    "  - TRAIN and TEST scores\n",
    "  - winner/inconclusive status\n",
    "- [ ] Export block-level table for bootstrap and consistency checks.\n",
    "- [ ] Save all diagnostic plots used in report.\n",
    "- [ ] Write concise conclusion text strictly following lock interpretation mapping.\n",
    "\n",
    "## 9) Definition of done\n",
    "\n",
    "- [ ] All lock constraints are satisfied exactly.\n",
    "- [ ] Winner decision is reproducible from saved tables and seed logs.\n",
    "- [ ] Mandatory validity checks are present for every participant.\n",
    "- [ ] If model recovery is weak, final claim is explicitly downgraded to weak/inconclusive evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# MODEL-COMPARISON LOCK (Triangle task; P01–P03; 4 blocks × 40 trials)\n",
    "\n",
    "## Goal (what I may conclude)\n",
    "I will conclude only: “Among these candidate models, under the pre-specified scoring + held-out evaluation + checks, model X provides the best predictive account of choice+RT in this dataset.”\n",
    "I will NOT conclude: “X is the true brain model.”\n",
    "\n",
    "---\n",
    "\n",
    "## Data + preprocessing (fixed)\n",
    "- Observables (targets): \n",
    "  1) `choice_t` (binary) \n",
    "  2) `rt_t` (continuous; milliseconds)\n",
    "- Inputs (given to models): `LLR_t`, hazard/block info, and any DNM-derived `psi_t` if applicable.\n",
    "- Exclusion rules (apply identically to all models):\n",
    "  - drop trials with missing choice or RT\n",
    "  - drop RT < 150 ms or RT > 5000 ms\n",
    "- Units: keep RT in milliseconds everywhere.\n",
    "\n",
    "---\n",
    "\n",
    "## Candidate models (fixed)\n",
    "Model A: **DNM + CNM (blockwise threshold)**\n",
    "- DNM provides trial-wise belief/prior quantities (e.g., `psi_t`) from hazard + evidence.\n",
    "- CNM uses a **block-specific threshold** parameter to generate choice+RT distribution.\n",
    "\n",
    "Model B: **DNM + CNM (blockwise asymptote)**\n",
    "- Same DNM inputs.\n",
    "- CNM uses a **block-specific asymptote (non-absorbing stabilization)** parameter to generate choice+RT distribution.\n",
    "\n",
    "Model C: **DNM + DDM (standard bounded diffusion)**\n",
    "- Map DNM outputs to DDM per trial:\n",
    "  - start point: `x0_t = k_z * psi_t`\n",
    "  - drift: `v_t = k_v * LLR_t`\n",
    "  - bounds: ±a, nondecision time: t0, diffusion scale fixed (see below).\n",
    "\n",
    "(If any mapping differs, that becomes a separate model and must be reported as such.)\n",
    "\n",
    "---\n",
    "\n",
    "## Fit vs fixed parameters (locked)\n",
    "Numerical hyperparameters (fixed for all models):\n",
    "- `dt = 1 ms`, `t_max = 5000 ms`, `n_sims_per_trial = 2000`\n",
    "- RNG: seeded and logged (default seed=0); same seed policy for all models.\n",
    "- RT density for likelihood: histogram density with bin width `20 ms` + epsilon smoothing `1e-12`.\n",
    "\n",
    "Model parameters to fit **per participant** (fit on TRAIN only):\n",
    "- Model A: 4 block params (threshold per block) + `t0` + one evidence/noise gain (single global).\n",
    "- Model B: 4 block params (asymptote per block) + `t0` + one evidence/noise gain (single global).\n",
    "- Model C (DDM): `a`, `t0`, `k_v`, `k_z`; diffusion scale fixed `s=1.0` (identifiability).\n",
    "\n",
    "---\n",
    "\n",
    "## Primary evaluation protocol (locked)\n",
    "Train/test split (forward-chaining; preserves sequential dependence):\n",
    "- For each block (40 trials):\n",
    "  - TRAIN = trials 1–30\n",
    "  - TEST  = trials 31–40\n",
    "- Fit parameters on TRAIN only; evaluate scores on TEST only.\n",
    "- Aggregate TEST scores across the 4 blocks per participant.\n",
    "\n",
    "History usage:\n",
    "- PRIMARY scoring uses one-step-ahead prediction: condition on observed history up to t−1 (if a model uses it).\n",
    "- SECONDARY validation uses free-running simulations (model feeds itself its own simulated history).\n",
    "\n",
    "---\n",
    "\n",
    "## Primary scoring rule (locked)\n",
    "For each trial t in TEST:\n",
    "- Joint negative log score:\n",
    "  - `L_t = -log p(choice_t)  -log p(rt_t | choice_t)`\n",
    "- `p(choice_t)` and `p(rt_t | choice_t)` are estimated from model simulations (same n_sims, dt, seed rules).\n",
    "Total score per participant = sum over TEST trials across all 4 blocks.\n",
    "Report also (but do NOT use for winner selection):\n",
    "- choice-only score = sum `-log p(choice_t)`\n",
    "- RT-only conditional score = sum `-log p(rt_t | choice_t)`\n",
    "\n",
    "Winner selection uses ONLY the joint score.\n",
    "\n",
    "---\n",
    "\n",
    "## “Winner” vs “inconclusive” rules (locked)\n",
    "Per participant:\n",
    "- A model is a clear winner if:\n",
    "  1) it has the best TEST joint score overall, AND\n",
    "  2) it is best in ≥ 3 of 4 blocks (blockwise consistency), AND\n",
    "  3) block-bootstrap over the 4 blocks gives ΔScore(best − runner-up) > 0 with 95% CI strictly > 0.\n",
    "Otherwise: “no clear winner” for that participant.\n",
    "\n",
    "Group-level (P01–P03):\n",
    "- Only claim a group preference if the same model is a clear winner in ≥ 2 of 3 participants.\n",
    "Otherwise: report heterogeneity / inconclusive.\n",
    "\n",
    "---\n",
    "\n",
    "## Mandatory validity checks (must be shown regardless of winner)\n",
    "1) Posterior predictive checks (per participant; per block):\n",
    "   - RT distribution overlay (data vs simulated)\n",
    "   - RT quantiles (10/30/50/70/90%)\n",
    "   - accuracy by block\n",
    "2) Change-point / hazard signatures (as plots):\n",
    "   - accuracy & RT near change-point vs later steady-state\n",
    "   - dependence of RT/choice on prior strength |psi| (if DNM present)\n",
    "3) Model recovery (surrogate data):\n",
    "   - simulate datasets from each fitted model; refit all models; check if generating model is recovered above chance.\n",
    "   - If recovery is poor, interpret any real-data “winner” as weak evidence (models not distinguishable here).\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation mapping (locked)\n",
    "If Model A wins: bounded/thresholded continuous accumulation with blockwise caution provides best predictive account.\n",
    "If Model B wins: non-absorbing stabilization/asymptote mechanism better captures behavior than strict bound crossing.\n",
    "If Model C wins: standard DDM driven by trial-wise prior (start) + evidence (drift) is sufficient; extra CNM nonlinearity not supported by prediction here.\n",
    "If inconclusive: dataset (3 participants; 4 blocks) does not disambiguate these mechanisms under the locked protocol; report equivalence and what data would be needed (more participants/blocks or stronger manipulations).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
